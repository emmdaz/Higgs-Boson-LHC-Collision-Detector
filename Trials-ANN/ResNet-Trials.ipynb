{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76709201-2fa4-4e3e-841f-38232164d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, AdamW\n",
    "from tensorflow.keras import regularizers, models, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "import gc\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190c3a2-802b-4512-beab-f69121510a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the GPU \\n\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba5067-29f3-4f6e-b8dc-a96da562948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "wandb.require(\"core\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b393777-7598-48c7-ad2e-785e7a4b08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the dataframe for all the data subsets for training, test and validation\n",
    "sg = pd.read_csv(\"/tf/Higgs-Boson-LHC-Collision-Detector/Datasets/Signal.csv\")\n",
    "bg = pd.read_csv(\"/tf/Higgs-Boson-LHC-Collision-Detector/Datasets/Background.csv\")\n",
    "\n",
    "sg_sample1 = sg.sample(frac = 0.5, random_state = 4)\n",
    "sg_sample2 = sg.sample(frac = 0.3, random_state = 4)\n",
    "sg_sample3 = sg.sample(frac = 0.2, random_state = 4)\n",
    "\n",
    "bg_sample1 = bg.sample(frac = 0.5, random_state = 4)\n",
    "bg_sample2 = bg.sample(frac = 0.3, random_state = 4)\n",
    "bg_sample3 = bg.sample(frac = 0.2, random_state = 4)\n",
    "\n",
    "train = pd.concat([sg_sample1, bg_sample1])\n",
    "test = pd.concat([sg_sample2, bg_sample2])\n",
    "val = pd.concat([sg_sample3, bg_sample3])\n",
    "\n",
    "# Now we can create the subsets:\n",
    "X_train, X_test, X_val = train.drop(columns = [\"label\"]), test.drop(columns = [\"label\"]), val.drop(columns = [\"label\"])\n",
    "y_train, y_test, y_val = train[\"label\"], test[\"label\"], val[\"label\"]\n",
    "\n",
    "# And we stardardize the data:\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check the sizes\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8410e-cdc6-4a6b-aeff-1437776606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the trials we make a smaller train and test sets:\n",
    "\n",
    "train_small = train.sample(frac = 0.15, random_state = 5)\n",
    "test_small = test.sample(frac = 0.15, random_state = 5)\n",
    "\n",
    "X_train_small, X_test_small = train_small.drop(columns = [\"label\"]), test_small.drop(columns = [\"label\"])\n",
    "y_train_small, y_test_small = train_small[\"label\"], test_small[\"label\"]\n",
    "\n",
    "X_train_small = scaler.fit_transform(X_train_small)\n",
    "X_test_small = scaler.transform(X_test_small)\n",
    "\n",
    "# Check the sizes\n",
    "print(\"Train size:\", len(X_train_small))\n",
    "print(\"Test size:\", len(X_test_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce2b45-39c0-4849-8225-1bd58576c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e59b8e-937f-4344-8355-2bf7c7ff0b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to create residual blocks (considering a layer regularizer L2)\n",
    "\n",
    "def residual_block(x, n, activation, dropout, dropout_rate, regularizer, r_2):\n",
    "        \n",
    "    residual = x  \n",
    "        \n",
    "    if dropout == \"y\":\n",
    "        # Camino \"principal\"\n",
    "        x = layers.Dense(n, activation = activation, kernel_regularizer = regularizers.l2(r_2))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Capa intermedia Dropot\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "            \n",
    "        # Capa lineal\n",
    "        x = layers.Dense(n, kernel_regularizer = regularizers.l2(r_2))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "            \n",
    "    else: \n",
    "        # Camino \"principal\"\n",
    "        x = layers.Dense(n, activation = activation, kernel_regularizer = regularizers.l2(r_2))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "            \n",
    "        # Capa lineal\n",
    "        x = layers.Dense(n, kernel_regularizer = regularizers.l2(r_2))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Suma de la conexi贸n residual\n",
    "    x = layers.add([x, residual]) \n",
    "    x = layers.Activation(activation)(x)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78a8e2-4f74-4aa7-bf50-d51661ea131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the function to start the study and optimizarion using Optuna\n",
    "def objective(trial):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inputs = layers.Input(shape = (X_train.shape[1],))\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    # Optuna suggests activation function for all layers\n",
    "    activation = trial.suggest_categorical(f\"activation_L{i+2}\", [\"relu\", \"relu6\", \"leaky_relu\"])\n",
    "    \n",
    "    # Optuna suggests regularizer L2 value\n",
    "    regularizer = \"L2\"\n",
    "    r_2 = trial.suggest_float(\"regularizer_value_2\", 1e-7, 1e-5, log = True)\n",
    "    \n",
    "    # Optuna suggest the number of layers\n",
    "    n_layers = trial.suggest_int(\"N_layers\", 15,20)\n",
    "    \n",
    "    # Optuna suggests learning rate value and an optimizer\n",
    "    lr = trial.suggest_float(\"learning_rate\", 2.5e-4, 1e-3, log = True)\n",
    "    \n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"adam\", \"rmsprop\", \"adamw\"])\n",
    "                              \n",
    "    if optimizer_name == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate = lr)              \n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    # First layer\n",
    "\n",
    "    # Optuna suggest number of neurons for the first layer\n",
    "\n",
    "    N = trial.suggest_int(\"N_1st_layer\", 128, 256)\n",
    "    \n",
    "    x = layers.Dense(N, input_shape = (X_train.shape[1],))(inputs)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Optuna suggests neurons for the residual blocks and if using Dropout block\n",
    "    \n",
    "    dropout_per_layer = []\n",
    "    dropout_percentage_per_layer = []\n",
    "            \n",
    "    dropping_out = trial.suggest_categorical(\"Dropout\", [\"y\", \"n\"])\n",
    "\n",
    "    N_per_layer = []\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        n = trial.suggest_int(f\"N_{i+1}_layer\", 128, 256)\n",
    "        N_per_layer.append(n)\n",
    "                              \n",
    "        dropout_rate = trial.suggest_float(f\"Dropout_value_L{i+2}\",0.1, 0.15)\n",
    "        \n",
    "        # i-th residual block:\n",
    "        \n",
    "        # Choosing between Dropout or a regulizer\n",
    "        \n",
    "        if dropping_out == \"y\":\n",
    "            dropout_percentage_per_layer.append(dropout_rate)\n",
    "            x = residual_block(x, n, activation, dropout, dropout_rate, regularizer, r_2)\n",
    "\n",
    "        else:\n",
    "            dropout_percentage_per_layer.append(0.0)\n",
    "            x = residual_block(x, n, activation, dropout, dropout_rate, regularizer, r_2)            \n",
    "            \n",
    "    x = layers.Dropout(0.4)(x)  \n",
    "    outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "                              \n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\",\n",
    "                             tf.keras.metrics.Precision(),\n",
    "                             tf.keras.metrics.AUC(curve = \"ROC\"),\n",
    "                             tf.keras.metrics.AUC(curve = \"PR\")])\n",
    "    \n",
    "    #############################################################################################################\n",
    "\n",
    "    wandb.init(\n",
    "        project = \"Residual-SnB-Trials-1.0\",\n",
    "        name = f\"Trial_{trial.number}\",\n",
    "        reinit = True,\n",
    "        config = {\n",
    "            \"Units_1\": N,\n",
    "            \"Units_per_layer\": N_per_layer,\n",
    "            \"activation\": activation,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"regularizer\": regularizer,\n",
    "            \"r_value2\": r_2,\n",
    "            \"Dropout\": dropping_out, \n",
    "            \"dropout_percentage_per_layer\": dropout_percentage_per_layer,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": activation,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    \"\"\"\n",
    "    Callbacks\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(monitor = \"val_accuracy\", patience = 10, restore_best_weights = True)\n",
    "    lr_reduction = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 7)\n",
    "#     tensorboard_cb = TensorBoard(log_dir = \"/workspace/Optuna-Trials/Plant-Pathology-Classificator/tf_debug\", histogram_freq = 1, write_graph = True,\n",
    "#                                  write_images = False)\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    \"\"\"\n",
    "    Creaci贸n del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(model.summary())\n",
    "    \n",
    "        history = model.fit(\n",
    "            X_train_small, y_train_small,\n",
    "            validation_data = (X_test_small, y_test_small),\n",
    "            epochs = 200,\n",
    "            verbose = 1, \n",
    "            callbacks = [WandbMetricsLogger(log_freq = 5), early_stopping, lr_reduction]\n",
    "        )\n",
    "\n",
    "        val_loss = min(history.history[\"val_loss\"])\n",
    "        val_accuracy = max(history.history[\"val_accuracy\"])\n",
    "        \n",
    "        train_loss = min(history.history[\"loss\"])\n",
    "        train_accuracy = max(history.history[\"accuracy\"])\n",
    "    \n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        \n",
    "        print(f\"Intento {trial.number} fall贸 debido a: {e}\")\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        wandb.finish()\n",
    "        gc.collect()\n",
    "        \n",
    "        return float(\"inf\")\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Intento {trial.number} fall贸. Unexpected error: {e}\")\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        wandb.finish()\n",
    "        gc.collect()\n",
    "        \n",
    "        return float(\"inf\")\n",
    "    \n",
    "    # score = val_loss + 0.1 * (train_loss - val_loss)\n",
    "    \n",
    "    score = val_accuracy\n",
    "    \n",
    "    # score = train_loss \n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    wandb.finish()\n",
    "\n",
    "    return 1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc04db8-11a1-4df3-874c-4d33797b0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name = \"Residual-Trials-1.0\",\n",
    "    direction = \"minimize\",\n",
    "    storage = \"sqlite:////workspace/Optuna-Trials/ResNet_SnB_study.db\",\n",
    "    load_if_exists = True\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials = 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
